# Classification-using-K-NN-and-Naive-Bayes
Classification with K-NN and Naive Bayes (Iris Dataset)
ğŸ¯ Objective
This mini-project applies supervised classification algorithms on the famous Iris flower dataset to:

Train models using K-Nearest Neighbors (K-NN) and Naive Bayes

Evaluate and compare their performance using standard classification metrics

ğŸ“ Dataset
Name: Iris Dataset

Source: [sklearn.datasets.load_iris()]

Features:

Sepal length, Sepal width

Petal length, Petal width

Target: Flower species (Setosa, Versicolor, Virginica)

ğŸ§° Tools Used
Python

Scikit-learn

Pandas

Seaborn & Matplotlib

ğŸ” Classification Models
K-Nearest Neighbors (K-NN)

Uses distance between points to classify a test sample

Value of k = 5

Gaussian Naive Bayes

Probabilistic classifier based on Bayes' theorem

Assumes features follow a Gaussian distribution

ğŸ“ˆ Evaluation Metrics
Each model is evaluated using:

âœ… Accuracy

âœ… Precision (macro-average)

âœ… Recall (macro-average)

âœ… Confusion Matrix (visualized with heatmaps)

ğŸ“Š Sample Results
Model	Accuracy	Precision	Recall
K-NN	0.98	0.98	0.98
Naive Bayes	0.96	0.96	0.96

âœ… Learning Outcomes
Understand and implement K-NN and Naive Bayes

Use standard metrics to evaluate classification models

Visualize results with confusion matrices

Compare different classification techniques
