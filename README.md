# Classification-using-K-NN-and-Naive-Bayes
Classification with K-NN and Naive Bayes (Iris Dataset)
🎯 Objective
This mini-project applies supervised classification algorithms on the famous Iris flower dataset to:

Train models using K-Nearest Neighbors (K-NN) and Naive Bayes

Evaluate and compare their performance using standard classification metrics

📁 Dataset
Name: Iris Dataset

Source: [sklearn.datasets.load_iris()]

Features:

Sepal length, Sepal width

Petal length, Petal width

Target: Flower species (Setosa, Versicolor, Virginica)

🧰 Tools Used
Python

Scikit-learn

Pandas

Seaborn & Matplotlib

🔍 Classification Models
K-Nearest Neighbors (K-NN)

Uses distance between points to classify a test sample

Value of k = 5

Gaussian Naive Bayes

Probabilistic classifier based on Bayes' theorem

Assumes features follow a Gaussian distribution

📈 Evaluation Metrics
Each model is evaluated using:

✅ Accuracy

✅ Precision (macro-average)

✅ Recall (macro-average)

✅ Confusion Matrix (visualized with heatmaps)

📊 Sample Results
Model	Accuracy	Precision	Recall
K-NN	0.98	0.98	0.98
Naive Bayes	0.96	0.96	0.96

✅ Learning Outcomes
Understand and implement K-NN and Naive Bayes

Use standard metrics to evaluate classification models

Visualize results with confusion matrices

Compare different classification techniques
